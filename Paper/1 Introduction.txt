Introduction

(I wrote this first but it's too verbose for the poster. Let's keep it for the paper, so scroll down for a shorter version for the poster)

Software development in HEP traditionally starts from use-cases and user-requirements. These are either derived from first principles or, often, from the need to overcome the inadequacies of an existing system. Data management tools are no exception, in particular for the CMS and ATLAS experiments at LHC, where data-movement is an integral part of the experiments' normal functioning.

Before and during Run-1 there was a strong emphasis on scaling up the performance, reliability and robustness of the data management tools. Each experiment has more than one system for moving data and managing it either in transit or on storage, with each system geared towards particular use-cases, requirements, or constraints. CMS has PhEDEx, AAA and ASO, while ATLAS has RUCIO and FAX (all of which are described below). These systems have been developed at different times and for different purposes, often by different teams even within the same experiment. All have followed, more or less, the standard use-case and requirements-driven development cycle, often tempered with experience with previous solutions.

One drawback of the standard methodology is that it is often hard to know what the requirements are, or will be at some point in even the near future. Changes in experiment data-models, in analysis behaviours, in available resources, the advent of new technologies such as multi-core processors or cloud-based computing platforms, these all make predicting future requirements extremely hard. As such, a system designed to match the postulated requirements of today may fail to cope with the situation in just a few years.

We propose a different way of looking at designing solutions for data management systems that steps back from this development methodology. We propose defining 'dimensions of data management' that describe the space of data management tools in terms of fundamental properties of data and the operations that can be performed on it. This we believe provides a framework for designing data management solutions that will be more flexible, so more easily evolved to meet unexpected changes in requirements in the future.

------------------------------------------------------
(I think this paragraph will be good enough for the poster)

HEP software is traditionally designed to satisfy certain use-cases or requirements, either based on a-priori estimates of the experiments' needs or on the shortcomings of an existing solution which no longer performs satisfactorily. However, designing against a requirements-document is only useful if the requirements are complete and stable.

Even if we could write down a complete set of requirements for data management systems for CMS and ATLAS, we cannot pretend that they will be stable over the course of several years. Experiments' data-models change, types and distributions of resources change, and the patterns of data-use also change. New technologies can disrupt the way experiments use data, such as the move to multi-core CPUs or cloud-based infrastructure. We should not be surprised if we discover that data management use-cases arise during Run-2 that we are not currently aware of, and that our systems may struggle to cope with.

We propose a framework for designing data management systems that takes a step back from the normal requirements-driven process and looks instead at data and the ways it can be manipulated. By identifying a set of 'dimensions of data management' we provide a way of breaking down the structure of data management systems into orthogonal components, whatever the use-cases they are built to serve. This makes the final product more flexible, more adaptable. New requirements will, we expect, map to only one or two of our dimensions, which means that the changes needed to support them will be well-contained, not spread throughout the code. New technologies, too, will be limited in their impact, because the technologies they replace are well isolated in the software.

This should lead to software designs which require less maintenance, do not become brittle over time, and which therefore last longer. Although it may appear to impose an overhead on the initial design of a new system, it should enable easier unit-testing of components and re-use of either code or at least the design of those components. It also provides a natural factorisation that can allow multiple developers to work on the same system coherently.