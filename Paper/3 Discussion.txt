Discussion

(This is a lot of text for a poster, let's keep it for the paper. There's a trimmed version below for the poster)

PhEDEx is the oldest of the data management solutions described here, and is the least well-aligned with the dimensions we define. It was designed well before either experiment took data, at a time when networks were considered to be the least reliable part of the computing infrastructure. This is all reflected in its design. It caters only to the mainstream experiment data, not to any arbitrary set of files that may need managing, so its data-flows are few and easily characterised; it features a static and stable network topology consisting of the well-maintained computing centres that make up the CMS collaboration; it is designed for reliability first, throughput second, while minimising latency is the lesser consideration. PhEDEx itself doesn't manage any file metadata beyond the location of each copy of a given file - i.e. it provides the global CMS file-location catalogue, but says nothing about the files' content or purpose.

While PhEDEx continues to serve the CMS experiment well, it is hard to adapt it to completely new use-cases that are now emerging (e.g. opportunistic resources, volunteer computing, desktop and laptop machines). Had it been designed along the dimensions we propose, adapting it for newer use-cases would be far easier.

AAA and ASO are both much newer, reaching production-readiness only in the last year or so. AAA is designed for low latency and high througput, operating on single files at a time. It provides access to a file for a batch job running on a remote site. As such, it's a data-access tool, not a data-movement tool, no permanent new copy of the file is stored at the destination site, though caching is certainly possible. AAA also provides an easy means for users to download single files to their private machines, so although the server-side topology is well defined, the destination-side topology is completely open. Because idle CPU is a wasted resource, performance is critical, and minimising latency is an important requirement.

ASO serves yet another purpose, moving files from batch jobs to their final destinations. Latency is important, since users will be waiting for their files before going on to the next phase of their analyses, but reliability is also paramount. The loss of a file means that a batch-job must be re-run, which is a waste of scarce CPU resources. ASO therefore has the reverse topology to deal with compared to AAA; the source can be batch-nodes anywhere inside (or even outside of) the CMS computing infrastructure, but the destinations are a set of stable, static sites. While AAA must concern itself with the load on the source-site, ASO takes care not to overload the destination-site.

While AAA and ASO appear to resemble each other more than either resembles PhEDEx, there are in fact many similarities between ASO and PhEDEx, due to the way files are collected for transport to the destination site. During the design of ASO the question was asked why we could not simply use PhEDEx, instead of building a new system. The answer can be seen in the dimensions of data management, PhEDEx was not cleanly factorised along lines that made it adaptable for this new use-case. While we were able to re-use parts of the design and code from PhEDEx there was still a fair amount of effort required to do this. Had we designed both products along the lines we suggest in this paper, less effort would have been required.

----------------------------------------------------------------------------------------------------
(shorter version, for the poster)

PhEDEx is the oldest system described here, designed at a time when networks were the least reliable part of the computing infrastructure. This shows in its design. It only manages the mainstream experiment data, not user- or anaylsis-data, so its data-flows are few and easily characterised; it has a static and stable network topology consisting of the T0/1/2 centres that belong to CMS; it is designed for reliability first, throughput second, while minimising latency is of less importance. PhEDEx itself only manages the location of each copy of a given file - i.e. it provides the global CMS file-location catalogue, but says nothing about the files' content or purpose.

PhEDEx is hard to adapt it to completely new use-cases (e.g. opportunistic resources, personal machines). Had it been designed along the dimensions we propose, adapting it for newer use-cases would be far easier.

AAA and ASO are much newer, reaching production-readiness only in the last year. AAA is designed for low latency and high througput, operating on single files at a time. It provides access to a file for a remote batch job. No permanent new copy of the file is stored, though caching is certainly possible. AAA also provides a means for users to download single files to private machines, so the server-side topology is well defined, but the destination-side topology is not. Minimising latency is important as CPU may be wasted waiting for the data.

ASO moves files from batch jobs to their final locations. ASO therefore has the reverse topology to deal with compared to AAA; the source can be batch-nodes anywhere inside (or even outside of) the CMS computing infrastructure, but the destinations are a set of stable, static sites. AAA must control the load on the source-site, while ASO must not overload the destination. Latency is important for ASO, but reliability is too. The loss of a file means a batch-job must be re-run, which is a waste of CPU.

AAA and ASO appear to resemble each other more than either resembles PhEDEx, but there are many similarities between ASO and PhEDEx, because it deals with bulk flows. Had PhEDEx been designed along the principles of our dimensions, it would have been easy to re-use many parts of it to build ASO. While we were able to re-use some parts of PhEDEx, this was not as easy as we would have liked.